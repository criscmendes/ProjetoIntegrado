{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom numpy import array\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\n\nfrom keras.layers import Dense,RNN,LSTM,Activation,Dropout\nfrom keras.models import Sequential\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.models import Model\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.feature_selection import SelectPercentile, chi2","metadata":{"execution":{"iopub.status.busy":"2021-06-18T13:11:54.961518Z","iopub.execute_input":"2021-06-18T13:11:54.962188Z","iopub.status.idle":"2021-06-18T13:12:03.083878Z","shell.execute_reply.started":"2021-06-18T13:11:54.962081Z","shell.execute_reply":"2021-06-18T13:12:03.082564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for replicability purposes\ntf.random.set_seed(91195003)\nnp.random.seed(91190530)\n#for an easy reset backend session state\ntf.keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T13:12:03.085986Z","iopub.execute_input":"2021-06-18T13:12:03.086478Z","iopub.status.idle":"2021-06-18T13:12:03.10983Z","shell.execute_reply.started":"2021-06-18T13:12:03.086418Z","shell.execute_reply":"2021-06-18T13:12:03.108572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv(\"../input/dataset/newsfeatures_withlabels.csv\")\ndf2 = pd.read_csv(\"../input/category/topic_cats.csv\")\ndf3 = pd.read_csv(\"../input/titleef-v2/items_title_EFv2.csv\")\n\n# descomentar para Métrica 2\n#dfM2 = pd.read_csv(\"../input/metricas/news_M2.csv\")\n#dfM2 = dfM2[['news_id', 'Toxic_Class']]\n\n# descomentar para Métrica 3\n#dfM3 = pd.read_csv(\"../input/metricas/news_M3.csv\")\n#dfM3 = dfM3[['news_id', 'Toxic_Class']]\n\n# descomentar para Métrica 4\n#dfM4 = pd.read_csv(\"../input/metricas/news_M4.csv\")\n#dfM4 = dfM4[['news_id', 'Toxic_Class']]\n\n# descomentar para Métrica 2\n#df1 = pd.merge(df1.drop(columns=['Toxic_Class']), dfM2, on='news_id', how='inner')\n\n# descomentar para Métrica 3\n#df1 = pd.merge(df1.drop(columns=['Toxic_Class']), dfM3, on='news_id', how='inner')\n\n# descomentar para Métrica 4\n#df1 = pd.merge(df1.drop(columns=['Toxic_Class']), dfM4, on='news_id', how='inner')\n\ndfaux = pd.merge(df1, df2, on = 'news_id', how='inner')\ndf = pd.merge(dfaux, df3, on = 'news_id', how='inner')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T13:12:03.112106Z","iopub.execute_input":"2021-06-18T13:12:03.112635Z","iopub.status.idle":"2021-06-18T13:12:06.348924Z","shell.execute_reply.started":"2021-06-18T13:12:03.112577Z","shell.execute_reply":"2021-06-18T13:12:06.347844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare Data\n'''\nBase + numero comentários + categoria\n\"^title_|time_of_day|newsoutlet_country|newsoutlet_name|num_comments_article|TC_\"\n\nBase + v1 + entidades corpo\n\"^title_|time_of_day|newsoutlet_country|newsoutlet_name|^freq_|text_\"\n\nBase + Keywords + v2\n\"^title_|time_of_day|newsoutlet_country|newsoutlet_name|[0-9]|noun_freq_\"\n'''\n\ndX = df.filter(regex=(\"^title_|time_of_day|newsoutlet_country|newsoutlet_name|[0-9]|noun_freq_\")).copy()\ndX","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:52:19.516432Z","iopub.execute_input":"2021-06-17T14:52:19.517007Z","iopub.status.idle":"2021-06-17T14:52:19.556269Z","shell.execute_reply.started":"2021-06-17T14:52:19.516969Z","shell.execute_reply":"2021-06-17T14:52:19.555048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn.feature_selection import SelectPercentile, chi2\n# param percentile indica percentagem de features que pretendemos manter\nfiltro = SelectPercentile(chi2, percentile=10).fit(dX, df['Toxic_Class'])\n# Devolve um array com a indicação das colunas a manter (True)\ncolunas = filtro.get_support(indices=False)\n\ndX = dX[dX.columns[colunas]]\ndX\n'''","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:35:44.716118Z","iopub.execute_input":"2021-06-17T11:35:44.716495Z","iopub.status.idle":"2021-06-17T11:35:44.722026Z","shell.execute_reply.started":"2021-06-17T11:35:44.716466Z","shell.execute_reply":"2021-06-17T11:35:44.721269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split data into training and validation sets\ndef split_data(training, perc=20):\n    train_idx = np.arange(0, int(len(training)*(100-perc)/100))\n    val_idx = np.arange(int(len(training)*(100-perc)/100+1), len(training))\n    return train_idx, val_idx","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:35:45.159094Z","iopub.execute_input":"2021-06-17T11:35:45.159452Z","iopub.status.idle":"2021-06-17T11:35:45.165549Z","shell.execute_reply.started":"2021-06-17T11:35:45.159422Z","shell.execute_reply":"2021-06-17T11:35:45.164063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_normalization(dataX, norm_range=(0, 1)):\n    scaler = MinMaxScaler(feature_range=norm_range)\n    \n    for c in dataX.columns:\n        dataX[[c]] = scaler.fit_transform(dataX[[c]])","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:35:46.158245Z","iopub.execute_input":"2021-06-17T11:35:46.158633Z","iopub.status.idle":"2021-06-17T11:35:46.164944Z","shell.execute_reply.started":"2021-06-17T11:35:46.1586Z","shell.execute_reply":"2021-06-17T11:35:46.163486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vizualizing Learning Curves \ndef plot_learning_curves(data, approach):\n\n    plt.figure(figsize=(9,3*n_splits))\n    if approach == 'history':\n        plt.title('Model train vs val loss per Training Split')\n        plt.ylabel('Training Accuracy (Normalized)')\n        plt.xlabel('Epoch')\n        for hist, i in zip(data, range(len(data))):\n            plt.subplot(n_splits,1,i+1)\n            plt.plot(hist.epoch, hist.history['loss'])\n            plt.plot(hist.epoch, hist.history['val_loss'])\n            plt.xlim([0, max(hist.epoch)])\n            plt.legend(['Training split ' + str(i+1) + '- train loss', 'Training split ' + str(i+1) + '- val loss' ])\n        plt.show()\n    elif approach == 'loss':\n        plt.figure(figsize=(6,3))\n        plt.plot(range(len(data)),data)\n        plt.title('Accuracy value per K Fold')\n        plt.ylabel('Evaluation Accuracy')\n        plt.xlabel('K Folds')\n        plt.xlim([0,2])\n        plt.ylim([0,(np.amax(data)+2)])\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:35:51.852386Z","iopub.execute_input":"2021-06-17T11:35:51.852729Z","iopub.status.idle":"2021-06-17T11:35:51.863086Z","shell.execute_reply.started":"2021-06-17T11:35:51.852698Z","shell.execute_reply":"2021-06-17T11:35:51.861968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(features, filters=16, kernel_size=5, pool_size=5, h_layers = 2):\n    #using the Functional API\n    inputs = tf.keras.layers.Input(shape=(features, 1))  \n\n    for i in range(h_layers):\n        x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, \n                                   activation='relu', data_format='channels_last', \n                                   padding='same', strides=1)(inputs)\n        x = tf.keras.layers.AveragePooling1D(pool_size=pool_size, \n                                             data_format='channels_first')(x)\n    #last layers\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(filters)(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='cnn_model')\n    model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:35:52.194335Z","iopub.execute_input":"2021-06-17T11:35:52.194884Z","iopub.status.idle":"2021-06-17T11:35:52.203081Z","shell.execute_reply.started":"2021-06-17T11:35:52.194835Z","shell.execute_reply":"2021-06-17T11:35:52.202123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compiling and fit the model\ndef compile_and_fit(model, epochs, batch_size):\n  \n    callbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n      #saving in Keras HDF5 (or h5), a binary data format\n      filepath='ckpt/my_model_{epoch}_{val_loss:.3f}.hdf5', #path where to save the model\n      save_best_only=True, #overwrite the current checkpoint if and only if\n      monitor='val_loss', #the val_loss score has improved\n      save_weights_only=False, #if True, only the weights are saved\n      verbose=1), #verbosity mode\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=20, min_delta=0.00001),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=50, min_lr=0.00005, cooldown=5)\n    ]\n\n    #fit\n    hist_list = list()\n    loss_list = list()\n\n    \n    #K Folds Validation\n    split_num = 1\n    kfold = StratifiedKFold(n_splits, shuffle=True, random_state=np.random.seed(seed))\n    \n    for train_idx, test_idx in kfold.split(dataX, datay):\n        train_idx, val_idx = split_data(train_idx, perc=10) #further split into training and validation sets\n\n        #build data\n        X_train, y_train = dataX[train_idx], datay[train_idx] \n        X_val, y_val = dataX[val_idx], datay[val_idx] \n        X_test, y_test = dataX[test_idx], datay[test_idx]\n\n        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n        X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n\n        print(f'\\n\\n## SPLIT {split_num} ##')\n        split_num += 1\n\n        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n                            epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n\n        metrics = model.evaluate(X_test, y_test)\n\n        predictions = model.predict(X_test)\n        print(f'Predictions: {predictions}\\n')\n        for i in range(len(predictions)):\n            if predictions[i] > 0.5:\n                predictions[i] = 1\n            else:\n                predictions[i] = 0\n        \n        #print(y_test.shape)\n        #print(predictions.shape)\n        print(classification_report(y_test, predictions, digits = 3))\n            \n        hist_list.append(history)\n        loss_list.append(metrics[1])\n\n\n    print(f'Accuracy LIST {loss_list} \\nMEAN: {np.mean(loss_list)}')\n\n    plot_learning_curves(hist_list, approach='history')\n    plot_learning_curves(loss_list, approach='loss')\n\n    return model, hist_list, loss_list, history","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:35:52.768001Z","iopub.execute_input":"2021-06-17T11:35:52.768348Z","iopub.status.idle":"2021-06-17T11:35:52.782247Z","shell.execute_reply.started":"2021-06-17T11:35:52.768308Z","shell.execute_reply":"2021-06-17T11:35:52.781114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Main Execution\nmultivariate = dX.shape[1] #number of features used by the model\nseed = 7\nn_splits = 7 #cross validator splits\nepochs = 100\nbatch_size = 64\n\ndata_normalization(dX)\ndataX=np.array(dX)\ndatay=np.array(df['Toxic_Class'])\n\n#fitting the model\nmodel = build_model(multivariate, filters = 32, kernel_size=5, pool_size=2, h_layers=5)\nmodel, hist_list, loss_list, history = compile_and_fit(model, epochs, batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:35:53.52645Z","iopub.execute_input":"2021-06-17T11:35:53.52711Z","iopub.status.idle":"2021-06-17T11:37:02.637997Z","shell.execute_reply.started":"2021-06-17T11:35:53.527065Z","shell.execute_reply":"2021-06-17T11:37:02.636957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testx = np.reshape(dataX, (dataX.shape[0], dataX.shape[1], 1))\ntestx.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:51:26.540151Z","iopub.execute_input":"2021-06-17T11:51:26.541019Z","iopub.status.idle":"2021-06-17T11:51:26.55161Z","shell.execute_reply.started":"2021-06-17T11:51:26.540945Z","shell.execute_reply":"2021-06-17T11:51:26.550539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inp = inp.reshape(1, timesteps, features)\nprev1 = model.predict(np.array([testx[0]]),verbose=1)\nprev1.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:51:26.846675Z","iopub.execute_input":"2021-06-17T11:51:26.847179Z","iopub.status.idle":"2021-06-17T11:51:27.025324Z","shell.execute_reply.started":"2021-06-17T11:51:26.847134Z","shell.execute_reply":"2021-06-17T11:51:27.024213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prev1","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:51:27.274515Z","iopub.execute_input":"2021-06-17T11:51:27.275048Z","iopub.status.idle":"2021-06-17T11:51:27.284382Z","shell.execute_reply.started":"2021-06-17T11:51:27.275004Z","shell.execute_reply":"2021-06-17T11:51:27.282728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inp = inp.reshape(1, timesteps, features)\nprev = model.predict(testx,verbose=1)\nprev.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:51:29.314093Z","iopub.execute_input":"2021-06-17T11:51:29.31472Z","iopub.status.idle":"2021-06-17T11:51:29.729662Z","shell.execute_reply.started":"2021-06-17T11:51:29.314654Z","shell.execute_reply":"2021-06-17T11:51:29.728683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}